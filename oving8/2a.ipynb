{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pygame\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# COLORS\n",
    "BLACK = (0, 0, 0)\n",
    "WHITE = (200, 200, 200)\n",
    "BLUE = (30, 144, 255)\n",
    "RED = (255, 0, 0)\n",
    "GREEN = (0, 255, 0)\n",
    "YELLOW = (255, 255, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Player:\n",
    "    def __init__(self,x,y,limit):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.limit = limit\n",
    "        self.color = BLUE\n",
    "\n",
    "    def move(self, direction):\n",
    "        # UP\n",
    "        if direction == 0 and self.y > 0:\n",
    "            self.y -= 1\n",
    "        # DOWN\n",
    "        elif direction == 1 and self.y < self.limit:\n",
    "            self.y += 1\n",
    "        # LEFT\n",
    "        elif direction == 2 and self.x > 0:\n",
    "            self.x -= 1\n",
    "        # RIGHT\n",
    "        elif direction == 3 and self.x < self.limit:\n",
    "            self.x += 1\n",
    "\n",
    "class Goal:\n",
    "    def __init__(self,x,y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.color = RED\n",
    "\n",
    "class Gridworld:\n",
    "    def __init__(self,rows, cols):\n",
    "        self.rows = 10\n",
    "        self.cols = 10\n",
    "\n",
    "        self.WINDOW_SIZE = 400\n",
    "        self.BLOCS_PER_ROW = 10\n",
    "        self.BLOCK_SIZE = self.WINDOW_SIZE / self.BLOCS_PER_ROW\n",
    "        self.PLAYER_SIZE = self.BLOCK_SIZE\n",
    "        self.PLAYER_X = 4\n",
    "        self.PLAYER_Y = 4\n",
    "        self.GOAL_X = 8\n",
    "        self.GOAL_Y = 8\n",
    "        self.TURN_TIME = 100\n",
    "\n",
    "        self.player = Player(self.PLAYER_X,self.PLAYER_Y,self.BLOCS_PER_ROW-1)\n",
    "        self.goal = Goal(self.GOAL_X,self.GOAL_Y)\n",
    "        self.action_space = 4\n",
    "        self.state = None\n",
    "        self.steps_taken = 0\n",
    "        self.screen = pygame.display.set_mode((self.WINDOW_SIZE, self.WINDOW_SIZE))\n",
    "\n",
    "    def step(self, action):\n",
    "        self.steps_taken += 1\n",
    "\n",
    "        # print(\"Step: %s - Action %s\" % (self.steps_taken, action))\n",
    "\n",
    "        self.player.move(action)\n",
    "        self.state = (self.player.x, self.player.y, self.goal.x, self.goal.y)\n",
    "\n",
    "        reward = 0\n",
    "        done = False\n",
    "        info = {\"direction\": action}\n",
    "        if self.player.x == self.goal.x and self.player.y == self.goal.y:\n",
    "            reward = 1\n",
    "            done = True\n",
    "\n",
    "        return np.array(self.state), reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        self.player = Player(self.PLAYER_X,self.PLAYER_Y,self.BLOCS_PER_ROW-1)\n",
    "        self.goal = Goal(self.GOAL_X,self.GOAL_Y)\n",
    "        self.state = (self.player.x, self.player.y, self.goal.x, self.goal.y)\n",
    "        self.steps_taken = 0\n",
    "\n",
    "        return np.array(self.state)\n",
    "\n",
    "    def draw_grid(self):\n",
    "        for row in range(self.BLOCS_PER_ROW):\n",
    "            for col in range(self.BLOCS_PER_ROW):\n",
    "                rect = pygame.Rect(self.BLOCK_SIZE * col,\n",
    "                                   self.BLOCK_SIZE * row,\n",
    "                                   self.BLOCK_SIZE,\n",
    "                                   self.BLOCK_SIZE)\n",
    "                pygame.draw.rect(self.screen, WHITE, rect, 1)\n",
    "\n",
    "    def draw_rect(self, x, y, color):\n",
    "        rect = pygame.Rect(x * self.BLOCK_SIZE, y * self.BLOCK_SIZE, self.PLAYER_SIZE, self.PLAYER_SIZE)\n",
    "        pygame.draw.rect(self.screen, color, rect)\n",
    "\n",
    "    def render(self, q_table):\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "                sys.exit()\n",
    "\n",
    "        # Reset backdrop\n",
    "        self.screen.fill(BLACK)\n",
    "        self.draw_grid()\n",
    "\n",
    "        # Q_table values\n",
    "        # self.draw_highest_Q_value(Q_table)\n",
    "        # self.render_table(q_table)\n",
    "\n",
    "        # Draw Player & Target\n",
    "        self.draw_rect(self.goal.x, self.goal.y, self.goal.color)\n",
    "        self.draw_rect(self.player.x, self.player.y, self.player.color)\n",
    "        pygame.display.flip()\n",
    "        pygame.time.wait(self.TURN_TIME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class GridWorldAgent:\n",
    "    def __init__(self, rows, cols, min_lr=0.1, min_er=0.1):\n",
    "        self.min_learning_rate = min_lr\n",
    "        self.min_exploration_rate = min_er\n",
    "        self.discount = 0.95\n",
    "        self.decay = 25\n",
    "        self.actions = {\"up\":0, \"right\":1, \"down\":2, \"left\":3}\n",
    "        self.action_space = [0,1,2,3]\n",
    "        self.q_table = np.zeros((rows, cols, len(self.action_space)))\n",
    "\n",
    "    def select_action(self, state, n):\n",
    "        decayed_er = self.decay_rate(n, self.min_exploration_rate)\n",
    "        if np.random.random() < decayed_er:\n",
    "            return random.randint(0, len(self.action_space)-1)\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state])\n",
    "\n",
    "    # Decays the learning rate so that the model learns more at the beginning.\n",
    "    def decay_rate(self, n, min_value):\n",
    "        return max(min_value, min(1.0, 1.0 - math.log10((n + 1) / self.decay)))\n",
    "\n",
    "    def update_q_table(self, state, action, reward, new_state, lr):\n",
    "        future_optimal_value = np.max(self.q_table[new_state])\n",
    "        learned_value = reward + self.discount * future_optimal_value\n",
    "        old_value = self.q_table[state][action]\n",
    "        self.q_table[state][action] = (1-lr)*old_value + lr*learned_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Started...\n",
      "Episode 0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "update_q_table() missing 1 required positional argument: 'lr'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-9-bd977c720abc>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     25\u001B[0m         \u001B[0;31m# Update Q-Table\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 26\u001B[0;31m         \u001B[0magent\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mupdate_q_table\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcurrent_state\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0maction\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mreward\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mnew_state\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     27\u001B[0m         \u001B[0;31m# Update state\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     28\u001B[0m         \u001B[0mcurrent_state\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnew_state\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: update_q_table() missing 1 required positional argument: 'lr'"
     ]
    }
   ],
   "source": [
    "training_episodes = 1000\n",
    "\n",
    "# Get Game environment\n",
    "env = Gridworld(rows=10, cols=10)\n",
    "agent = GridWorldAgent(rows=env.rows, cols=env.cols,min_lr=0.01,min_er=0.1)\n",
    "\n",
    "# Initial Q-table\n",
    "q_table = np.zeros((10, 10) + (env.action_space,))\n",
    "\n",
    "# Training\n",
    "print(\"Training Started...\")\n",
    "for episode in range(training_episodes):\n",
    "\n",
    "    lr = agent.decay_rate(episode,agent.min_learning_rate)\n",
    "\n",
    "    print(\"Episode %s\" % episode)\n",
    "    current_state = env.reset()\n",
    "    done = False\n",
    "    for t in range(100):\n",
    "        # Decide known action or random action.\n",
    "        action = agent.select_action(current_state, episode)\n",
    "\n",
    "        # Increment environment\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        new_state = obs\n",
    "\n",
    "        # Update Q-Table\n",
    "        agent.update_q_table(current_state,action,reward,new_state,lr)\n",
    "        # Update state\n",
    "        current_state = new_state\n",
    "\n",
    "        if episode > 800:\n",
    "            env.render(q_table)\n",
    "\n",
    "        if done:\n",
    "            print(\"Success after %s steps\" % t)\n",
    "            break\n",
    "    #print(\"Steps taken: %dt\" % env.steps_taken)\n",
    "\n",
    "print(\"Training Complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}